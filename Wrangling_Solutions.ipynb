{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm2e6uEM2oA8Jav5AxugZo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EllaThomasson/EDA/blob/main/Wrangling_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1"
      ],
      "metadata": {
        "id": "GE92rqwxBY9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The author of this paper emphasizes the importance of having clean data. This paper generally seems to be about making a clear definition of data cleaning. Wickham data is clean when each row has exactly one observation, when each column houses exactly one variable, and when each type of observational unit is a table.\n",
        "\n",
        "2. The tidy data standard was designed to facilitate initial exploration and analysis of the data. It is meant to make the data cleaning process easier by helping tools work well together. This allows you to focus on more interesting domain problems.\n",
        "\n",
        "3. The first quote means that organized or well-structured datasets share common characteristics, like being clean, complete, and easy to analyze. However, messy datasets each have their own unique issues, like missing values, inconsistencies, or noise—making them distinct in their chaos. The second sentence suggests that while it's often straightforward to identify the elements of a specific dataset—such as which items are observations and which are variables—it can be challenging to provide a clear and universal definition of what constitutes observations and variables across different contexts or datasets.\n",
        "\n",
        "4. A dataset is a collection of values. A variable measures the same attribute of all observations. An observation contains all values measured on the same unit.\n",
        "\n",
        "5. Tidy data is where each variable forms a column, each observation forms a row, and each type of observational unit forms a table.\n",
        "\n",
        "6. Some common problems with messy data are: 1. Column headers are values and not names. In this case, variables form both the rows and the columns, and column headers are values, not variable names. This forms a problem. 2. There are multiple variables stored in one column. This often happens after melting, where the column variable names often become a combination of multiple underlying variable names. 3. Variables are stored in both rows and columns. This is the most complex form of messy data. 4. Multiple types in one table. During tidying, each type of observational unit should be stored in its own table. 5. One type in multiple tables. Generally a pretty easy problem to fix. In table 4, income should be a variable, not shown as values across the columns. In this case, the column headers are values and not names. Melting is the process of fixing this issue. It is the process of converting column-value variables into rows.\n",
        "\n",
        "7. For starters, table 11 has an issue with values being in the columns. More specifically, the days are in the columns, which need to be brought down into a row. This is fixed in table 12, and a date variable was made combining month, year, and day. Table 12 made the values into one column as well, which is good. Overall, table 12 is good because each column is a variable and each row is an observation. This was not the case with table 11.\n",
        "\n",
        "8. The chicken and egg problem with focusing on tidy data is that if tidy data is only as useful as the tools that work with it, then tidy tools will always be linked to tidy data with no way out. Because of this, it is easy to get stuck in a local maxima where independently changing data structures/tools will not improve workflow. Breaking out of this local maxima will take long-term consistent effort. In the future, Wickham hopes that the tidy framework isn't a false start and that people continue to grow it and build upon it. Additionally, Wickham wants the tidy concept to be bigger than just technical concepts. He wants it to be more about the bigger ideas and tools for data science more generally.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jc_YXj5RBbdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "85iWnJs9gkLQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2"
      ],
      "metadata": {
        "id": "glXvIunUBVoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/DS3001/wrangling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl2Mo5DxhIMn",
        "outputId": "4b5535d5-23dd-4d2b-d707-90c435855a87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wrangling'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 92 (delta 36), reused 17 (delta 14), pack-reused 43 (from 1)\u001b[K\n",
            "Receiving objects: 100% (92/92), 18.19 MiB | 6.97 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1"
      ],
      "metadata": {
        "id": "zFuefqtZB4yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb = pd.read_csv('./wrangling/assignment/data/airbnb_hw.csv', low_memory=False)\n",
        "\n",
        "# first took a look at the data type / values\n",
        "\n",
        "airbnb['Price'].value_counts().sort_values()\n",
        "\n",
        "(airbnb['Price']).unique() # here I can see that the observations are recorded as strings because there is a comma in one + numbers\n",
        "\n",
        "# so I need to take out the commas and change the column to numeric\n",
        "\n",
        "airbnb['Price'] = airbnb['Price'].str.replace(',','')\n",
        "\n",
        "pd.to_numeric(airbnb['Price'])\n",
        "\n",
        "\n",
        "# now I'm taking a look at missing values\n",
        "\n",
        "airbnb['Price'].isnull().sum() # there are no missing values, so we are all good here\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcjhXPce-bTt",
        "outputId": "199bbb20-c586-4e58-c11a-d58557dca219"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2"
      ],
      "metadata": {
        "id": "fpwnBD98CeK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sharks = pd.read_csv('./wrangling/assignment/data/sharks.csv', low_memory=False)\n",
        "\n",
        "# again, first I'm taking a look at all the values\n",
        "\n",
        "sharks['Type'].value_counts()\n",
        "\n",
        "## I see that some values are the same\\similar, so I will combine them\n",
        "\n",
        "sharks['Type'] = sharks['Type'].replace('Boating', 'Boat')\n",
        "sharks['Type'] = sharks['Type'].replace('Boatomg', 'Boat')\n",
        "sharks['Type'] = sharks['Type'].replace('Sea Disaster', 'Boat')\n",
        "sharks['Type'] = sharks['Type'].replace('Watercraft', 'Boat')\n",
        "sharks['Type'].value_counts()\n",
        "\n",
        "## all of the following variables are unknown for some reason, so I will make them all NA\n",
        "\n",
        "sharks['Type'] = sharks['Type'].replace(['Invalid', 'Questionable','Unconfirmed','Unverified','Under investigation'],np.nan)\n",
        "sharks['Type'].value_counts()\n",
        "\n",
        "\n",
        "## I will not remove the NAs yet, because it may be useful to know when the \"type\" was questionable.\n"
      ],
      "metadata": {
        "id": "6dePHEJDTZn6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3"
      ],
      "metadata": {
        "id": "yj6LKmSgChhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "justice = pd.read_parquet('./wrangling/assignment/data/justice_data.parquet')\n",
        "\n",
        "# taking a look at the data first\n",
        "\n",
        "justice['WhetherDefendantWasReleasedPretrial'].unique() # 9 means \"unclear\" ... so lets change that to NAN\n",
        "\n",
        "justice['WhetherDefendantWasReleasedPretrial'] = justice['WhetherDefendantWasReleasedPretrial'].replace(9,np.nan)\n",
        "\n",
        "# double checking to make sure we fixed the problem\n",
        "\n",
        "justice['WhetherDefendantWasReleasedPretrial'].value_counts() # this looks good!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "YIr98v2FTdqJ",
        "outputId": "e8665a4f-5181-407a-ef43-6cfb300df9dc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WhetherDefendantWasReleasedPretrial\n",
              "1.0    19154\n",
              "0.0     3801\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WhetherDefendantWasReleasedPretrial</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>19154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>3801</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4"
      ],
      "metadata": {
        "id": "frQp-a5YCjJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pretrial = pd.read_csv('./wrangling/pretrial_data.csv', low_memory=False)\n",
        "\n",
        "pretrial['ImposedSentenceAllChargeInContactEvent'] = pd.to_numeric(pretrial['ImposedSentenceAllChargeInContactEvent'],errors='coerce')\n",
        "\n",
        "pretrial['ImposedSentenceAllChargeInContactEvent'].isnull().sum() / 22986\n",
        "# almost 40% NA values\n",
        "\n",
        "subset = pretrial[['ImposedSentenceAllChargeInContactEvent','SentenceTypeAllChargesAtConvictionInContactEvent']]\n",
        "\n",
        "\n",
        "# after looking at the data, it appears as though when the SentenceTypeAllChargesAtConvictionInContactEvent\n",
        "# is 4 and 9, the ImposedSentenceAllChargeInContactEvent is more likely to be missing\n",
        "\n",
        "\n",
        "# Category 4 is cases where the charges were dismissed therefore this value should be 0 not NA\n",
        "\n",
        "pretrial['ImposedSentenceAllChargeInContactEvent'] = pretrial['ImposedSentenceAllChargeInContactEvent'].mask( pretrial['SentenceTypeAllChargesAtConvictionInContactEvent'] == 4, 0)\n",
        "\n",
        "\n",
        "subset = pretrial[['ImposedSentenceAllChargeInContactEvent','SentenceTypeAllChargesAtConvictionInContactEvent']]\n",
        "\n",
        "\n",
        "# 9s should actually be NA\n",
        "\n",
        "subset['ImposedSentenceAllChargeInContactEvent'].isnull().sum() / 22986\n",
        "\n",
        "# now there are only 274 missing values ... which is only around 1% of the data, much better\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUYcMIaqCkMN",
        "outputId": "4033cbaf-431e-4dda-9007-03195266a6cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.011920299312625076"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}